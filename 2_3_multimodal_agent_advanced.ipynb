{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af3794b",
   "metadata": {},
   "source": [
    "# Multimodal Agent (Advanced): Run EXAONE Atelier from AWS Marketplace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ea02ec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK to deploy a multimodal agent using EXAONE Atelier Image Captioning Model and Large Language Models (LLMs) from AWS Marketplace. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35642ab2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b55e677-3429-4668-b100-bd63d2a4c401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet sagemaker\n",
    "%pip install --upgrade --quiet sagemaker accelerate datasets tritonclient[all] gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014424a8-7f8f-46a7-8963-2c3d454878b8",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdVersion"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "#model_package = \"exaone-atelier-i2t-limited-85f3f0d181593a10b7aef9bea522a333\" # EXAONE Atelier - Image to Text - Limited\n",
    "model_package = \"exaone-atelier-i2t-76c77246a8343a23a36b2ce80c06f4f6\" # EXAONE Atelier - Image to Text\n",
    "model_package_map = {\n",
    "    \"us-east-1\": f\"arn:aws:sagemaker:us-east-1:865070037744:model-package/{model_package}\",\n",
    "    \"us-east-2\": f\"arn:aws:sagemaker:us-east-2:057799348421:model-package/{model_package}\",\n",
    "    \"us-west-1\": f\"arn:aws:sagemaker:us-west-1:382657785993:model-package/{model_package}\",\n",
    "    \"us-west-2\": f\"arn:aws:sagemaker:us-west-2:594846645681:model-package/{model_package}\",\n",
    "    \"ca-central-1\": f\"arn:aws:sagemaker:ca-central-1:470592106596:model-package/{model_package}\",\n",
    "    \"eu-central-1\": f\"arn:aws:sagemaker:eu-central-1:446921602837:model-package/{model_package}\",\n",
    "    \"eu-west-1\": f\"arn:aws:sagemaker:eu-west-1:985815980388:model-package/{model_package}\",\n",
    "    \"eu-west-2\": f\"arn:aws:sagemaker:eu-west-2:856760150666:model-package/{model_package}\",\n",
    "    \"eu-west-3\": f\"arn:aws:sagemaker:eu-west-3:843114510376:model-package/{model_package}\",\n",
    "    \"eu-north-1\": f\"arn:aws:sagemaker:eu-north-1:136758871317:model-package/{model_package}\",\n",
    "    \"ap-southeast-1\": f\"arn:aws:sagemaker:ap-southeast-1:192199979996:model-package/{model_package}\",\n",
    "    \"ap-southeast-2\": f\"arn:aws:sagemaker:ap-southeast-2:666831318237:model-package/{model_package}\",\n",
    "    \"ap-northeast-2\": f\"arn:aws:sagemaker:ap-northeast-2:745090734665:model-package/{model_package}\",\n",
    "    \"ap-northeast-1\": f\"arn:aws:sagemaker:ap-northeast-1:977537786026:model-package/{model_package}\",\n",
    "    \"ap-south-1\": f\"arn:aws:sagemaker:ap-south-1:077584701553:model-package/{model_package}\",\n",
    "    \"sa-east-1\": f\"arn:aws:sagemaker:sa-east-1:270155090741:model-package/{model_package}\",\n",
    "}\n",
    "region = boto3.Session().region_name\n",
    "if region not in model_package_map.keys():\n",
    "    raise Exception(f\"Current boto3 session region {region} is not supported.\")\n",
    "\n",
    "model_package_arn = model_package_map[region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01729938-4f1c-4ddf-bef4-63a314909eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'exaone-atelier-i2t'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7029716d",
   "metadata": {},
   "source": [
    "In this demo notebook, we use multiple crops of one image to provide LLMs with more detailed information about the image.\n",
    "\n",
    "We recommend using `p4d.24xlarge` for faster image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e52afae-868d-4736-881f-7180f393003a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import ModelPackage\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "\n",
    "model = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=model_package_arn,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.p4d.24xlarge',\n",
    "    endpoint_name=model_name,\n",
    "    container_startup_health_check_timeout=3600,\n",
    ")\n",
    "model.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edc6ac3",
   "metadata": {},
   "source": [
    "---\n",
    "Here we use Meta's Llama-2 7B for LLM.\n",
    "\n",
    "To perform inference on Llama-2 models, you need to pass custom_attributes='accept_eula=true' as part of header. This means you have read and accept the end-user-license-agreement (EULA) of the model. EULA can be found in model card description or from https://ai.meta.com/resources/models-and-libraries/llama-downloads/. By default, this notebook sets custom_attributes='accept_eula=false', so all inference requests will fail until you explicitly change this custom attribute.\n",
    "\n",
    "---\n",
    "You can also choose between:\n",
    "\n",
    "- `meta-textgeneration-llama-2-7b-f`\n",
    "- `meta-textgeneration-llama-2-13b-f`\n",
    "- `meta-textgeneration-llama-2-70b-f`\n",
    "\n",
    "We haven't tested on other LLMs provided in JumpStart.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911233bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-2-7b-f\", \"3.0.0\"\n",
    "#model_id, model_version = \"meta-textgeneration-llama-2-13b-f\", \"3.0.0\"\n",
    "#model_id, model_version = \"meta-textgeneration-llama-2-70b-f\", \"3.0.0\"\n",
    "\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "llm_model = JumpStartModel(model_id=model_id, model_version=model_version)\n",
    "llm_predictor = llm_model.deploy(accept_eula=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40991627",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"suggest a good place to eat.\"\n",
    "\n",
    "llm_payload = {\n",
    "    \"inputs\": f\"[INST] {prompt} [/INST] \",    \n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6},\n",
    "}\n",
    "\n",
    "response = llm_predictor.predict(llm_payload, custom_attributes=\"accept_eula=true\") # set accept_eula to true\n",
    "\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef7207e-01ba-4ac2-b4a9-c8f6f0e1c498",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b1a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import tritonclient.http as httpclient\n",
    "import requests\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "def encode_image(image):\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format=\"JPEG\")\n",
    "    img_str = base64.b64encode(buffer.getvalue())\n",
    "    return img_str\n",
    "\n",
    "def get_sample_binary(payload):\n",
    "\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for idx, dic in enumerate(payload[\"inputs\"]):\n",
    "        input_name = dic[\"name\"]\n",
    "        input_value = dic[\"data\"][0]\n",
    "\n",
    "        input_value = np.array([input_value.encode('utf-8')], dtype=np.object_)\n",
    "\n",
    "        input_value = np.expand_dims(input_value, axis=0)\n",
    "        inputs.append(httpclient.InferInput(input_name, [1, 1], \"BYTES\"))\n",
    "        inputs[idx].set_data_from_numpy(input_value)\n",
    "\n",
    "    outputs.append(httpclient.InferRequestedOutput(\"generated_caption\", binary_data=True))\n",
    "\n",
    "    request_body, header_length = httpclient.InferenceServerClient.generate_request_body(\n",
    "        inputs, outputs=outputs\n",
    "    )\n",
    "    return request_body, header_length\n",
    "\n",
    "\n",
    "def invoke_endpoint(endpoint_name, payload):\n",
    "    import re\n",
    "    request_body, header_length = get_sample_binary(payload)\n",
    "    response = smr_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/vnd.sagemaker-triton.binary+json;json-header-size={}\".format(\n",
    "            header_length\n",
    "        ),\n",
    "        Body=request_body\n",
    "    )\n",
    "    data = response[\"Body\"].read()\n",
    "    ptn = re.compile(rb'\\{\"binary_data_size\":[0-9]*\\}')\n",
    "    match = json.loads(ptn.search(data).group().decode('utf-8'))\n",
    "    binary_data_size = match['binary_data_size']\n",
    "    binary_data = data[len(data)-binary_data_size+1:]\n",
    "    binary_data = binary_data.replace(b'\\x00', b'')\n",
    "    binary_data = binary_data.replace(b'\\x01', b'').decode('utf-8')    \n",
    "\n",
    "    return eval(binary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27007cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image(image, pro_mode=False, max_size = 2048):\n",
    "    width, height = image.size\n",
    "    if width < 256 or height < 256:       \n",
    "        raw = image.resize((256, 256)) \n",
    "\n",
    "    else:\n",
    "        resize_dim = min(width, height)\n",
    "        raw = image.resize((resize_dim, resize_dim))    \n",
    "    if pro_mode:   \n",
    "        width, height = image.size\n",
    "        if width < max_size or height < max_size:\n",
    "            if width > height:\n",
    "                ratio = width / height         \n",
    "                image = image.resize((int( max_size * ratio), max_size)) \n",
    "            else:\n",
    "                ratio = height / width\n",
    "                image = image.resize((max_size, int( max_size * ratio)))    \n",
    "        image.thumbnail((max_size, max_size))\n",
    "\n",
    "        width, height = image.size\n",
    "        crop_dim = round(min(width, height) * 2/3)\n",
    "        #crop_dim = min(crop_dim, (max(width, height) - crop_dim) * 2/3)\n",
    "        \n",
    "        #center crop\n",
    "        left = round((width - crop_dim)/2)\n",
    "        top = round((height - crop_dim)/2)\n",
    "        x_right = round(width - crop_dim) - left\n",
    "        x_bottom = round(height - crop_dim) - top\n",
    "        right = width - x_right\n",
    "        bottom = height - x_bottom\n",
    "        center = image.crop((left, top, right, bottom))  \n",
    "        \n",
    "        crop_dim = min(crop_dim, (max(width, height) - crop_dim) * 2/3)        \n",
    "        #upper left \n",
    "        left = 0\n",
    "        top = 0\n",
    "        right = left + crop_dim\n",
    "        bottom = top + crop_dim\n",
    "        upper_left = image.crop((left, top, right, bottom))   \n",
    "    \n",
    "        #upper right\n",
    "        left = width - crop_dim\n",
    "        top = 0\n",
    "        right = left + crop_dim\n",
    "        bottom = top + crop_dim\n",
    "        upper_right = image.crop((left, top, right, bottom))   \n",
    "\n",
    "        #lower left \n",
    "        left = 0\n",
    "        top = height - crop_dim\n",
    "        right = left + crop_dim\n",
    "        bottom = top + crop_dim\n",
    "        lower_left = image.crop((left, top, right, bottom))   \n",
    "\n",
    "        #lower right \n",
    "        left = width - crop_dim\n",
    "        top = height - crop_dim\n",
    "        right = left + crop_dim\n",
    "        bottom = top + crop_dim\n",
    "        lower_right = image.crop((left, top, right, bottom)) \n",
    "        return [raw, center, upper_left, upper_right, lower_left, lower_right]\n",
    "    else:\n",
    "        return [raw,]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a4f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input(image):\n",
    "    input_image = encode_image(image)\n",
    "    inputs = dict(\n",
    "        image=input_image,\n",
    "    )\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            {\"name\": name, \"shape\": [1, -1], \"datatype\": \"BYTES\", \"data\": [data.decode('utf8')]}\n",
    "            for name, data in inputs.items()\n",
    "        ]\n",
    "    }\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94479150",
   "metadata": {},
   "source": [
    "## Run Multimodal Agent with Gradio\n",
    "---\n",
    "In this example, we use Gradio to run an demo page of actual multimodal agent. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe14713",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import re\n",
    "endpoint_name = model.endpoint_name\n",
    "\n",
    "def remove_response_tags(text):\n",
    "    # Using a regular expression to match 'response' and its variations in various formats and case insensitivities\n",
    "    # The pattern matches 'response' with optional leading and trailing characters like '[' or ':'\n",
    "    # This version includes common misspellings like 'responce'\n",
    "    cleaned_text = re.sub(r'\\[?\\bRESPONSE\\b\\]?:?', '', text, flags=re.IGNORECASE)\n",
    "    cleaned_text = re.sub(r'\\[?\\bRESPONCE\\b\\]?:?', '', cleaned_text, flags=re.IGNORECASE)\n",
    "    cleaned_text = re.sub(r'\\[?\\bREPSONSE\\b\\]?:?', '', cleaned_text, flags=re.IGNORECASE)\n",
    "    cleaned_text = re.sub(r'\\[?\\bREPSONCE\\b\\]?:?', '', cleaned_text, flags=re.IGNORECASE)    \n",
    "    return cleaned_text\n",
    "\n",
    "def captioning_fn(image, pro_mode=False):\n",
    "    images = prepare_image(image, pro_mode)   \n",
    "    result = []\n",
    "    for image in images:\n",
    "        result.append(invoke_endpoint(endpoint_name, generate_input(image))[0])            \n",
    "    \n",
    "    return result, 'image upload finished.'\n",
    "\n",
    "def chat_fn(captions, question, pro_mode=False, return_prompt=False):\n",
    "    if len(captions) == 1:\n",
    "        pro_mode = False\n",
    "    if pro_mode:\n",
    "        prompt_prefix = f\"\"\"Here's a description of an image.\n",
    "        \n",
    "{captions[0][0]}\n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "        prompt_body = \"\"\"Interpret the following elements as different aspects of the same image. \n",
    "Each element comes with its position.\\n\\n\"\"\"\n",
    "        \n",
    "        baseline = captions[0][1]\n",
    "        extra = 0\n",
    "        if captions[1][1] >= baseline:        \n",
    "            prompt_body += f\"Center: {captions[1][0]}\\n\" \n",
    "            extra += 1\n",
    "        if captions[2][1] >= baseline:\n",
    "            prompt_body += f\"Upper Left: {captions[2][0]}\\n\"\n",
    "            extra += 1            \n",
    "        if captions[3][1] >= baseline:\n",
    "            prompt_body += f\"Upper Right: {captions[3][0]}\\n\"\n",
    "            extra += 1            \n",
    "        if captions[4][1] >= baseline:\n",
    "            prompt_body += f\"Lower Left: {captions[4][0]}\\n\"\n",
    "            extra += 1            \n",
    "        if captions[5][1] >= baseline:\n",
    "            prompt_body += f\"Lower Right: {captions[5][0]}\\n\"\n",
    "            extra += 1            \n",
    "        prompt_body += \"\"\"\\nEach of these elements contributes to the overall understanding of the image. \n",
    "Consider them collectively to form a unified interpretation of the image.\\n\\n\"\"\"\n",
    "        \n",
    "        prompt_suffix = f\"\"\"Based on your understanding of the image, provide a response to the following request:\n",
    "\n",
    "{question} \n",
    "\n",
    "Do not include your interpretation to the response.\n",
    "Start your response with [RESPONSE].\"\"\"\n",
    "        \n",
    "        prompt = prompt_prefix\n",
    "        if extra > 0:\n",
    "            prompt += prompt_body\n",
    "        prompt += prompt_suffix\n",
    "    else:\n",
    "        prompt = f\"\"\"Here's a description of an image.\n",
    "\n",
    "{captions[0][0]}\n",
    "\n",
    "Based on your understanding of the image, provide a response to the following request:\n",
    "\n",
    "{question} \n",
    "\n",
    "Start your response with [RESPONSE].\"\"\"        \n",
    "        \n",
    "    llm_payload = {\n",
    "    \"inputs\": f\"[INST] {prompt} [/INST] \",    \n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.92, \"temperature\": 0.6},\n",
    "    }\n",
    "    \n",
    "    response = llm_predictor.predict(llm_payload, custom_attributes=\"accept_eula=true\") # set accept_eula to true\n",
    "\n",
    "    if return_prompt:\n",
    "        return prompt, remove_response_tags(response[0]['generated_text']).strip()\n",
    "    else:\n",
    "        return remove_response_tags(response[0]['generated_text']).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1e783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/LGAI-Research/EXAONE-Atelier/blob/main/images/example_1.png?raw=true'\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert('RGB') \n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--NORMAL MODE--')\n",
    "captions, message = captioning_fn(image, False)\n",
    "print(captions)\n",
    "print('--------------')\n",
    "prompt, response = chat_fn(captions, \"Describe the image.\", False, True)\n",
    "print(prompt)\n",
    "print('--------------')\n",
    "print(response)\n",
    "print('---PRO MODE---')\n",
    "captions, message = captioning_fn(image, True)\n",
    "print(captions)\n",
    "print('--------------')\n",
    "prompt, response = chat_fn(captions, \"Describe the image.\", True, True)\n",
    "print(prompt)\n",
    "print('--------------')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cc87d5",
   "metadata": {},
   "source": [
    "### Run demo\n",
    "---\n",
    "Before you upload an image, you can choose to turn `Pro Mode` on by clicking check box. Everytime you change the mode, press the `Upload` button again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef85042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(css=\"\"\"\n",
    "    .message.svelte-w6rprc.svelte-w6rprc.svelte-w6rprc {font-size: 20px; margin-top: 20px}\n",
    "    #component-21 > div.wrap.svelte-w6rprc {height: 600px;}\n",
    "    \"\"\") as demo:\n",
    "        title = \"\"\"<h1 align=center>EXAONE Atelier Multimodal Agent</h1>\"\"\"\n",
    "        gr.Markdown(title)       \n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                input_image = gr.Image(type=\"pil\")   \n",
    "                with gr.Row():\n",
    "                    pro_mode = gr.Checkbox(label=\"Pro Mode\")\n",
    "                    predict_btn = gr.Button(value=\"Upload\")   \n",
    "                with gr.Row():      \n",
    "                    req_prompt = gr.Textbox(label=\"Input\")                     \n",
    "                llm_btn = gr.Button(value=\"Generate\")                                            \n",
    "                                  \n",
    "            with gr.Column():\n",
    "                best_cap = gr.State()\n",
    "                answer = gr.Textbox(label=\"Output\")  \n",
    "        predict_btn.click(\n",
    "            captioning_fn,\n",
    "            inputs=[\n",
    "                input_image,\n",
    "                pro_mode,\n",
    "            ],\n",
    "            outputs=[best_cap, answer])                 \n",
    "        llm_btn.click(\n",
    "            chat_fn,\n",
    "            inputs=[best_cap,\n",
    "                req_prompt,\n",
    "                pro_mode,\n",
    "            ],\n",
    "            outputs=[answer])   \n",
    "\n",
    "demo.queue().launch(auth=(\"user\", \"test1234\"), share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e062d29",
   "metadata": {},
   "source": [
    "## Clean Up the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cc5560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "#model.sagemaker_session.delete_endpoint(model.endpoint_name)\n",
    "#model.sagemaker_session.delete_endpoint_config(model.endpoint_name)\n",
    "#model.delete_model()\n",
    "\n",
    "#llm_predictor.delete_model()\n",
    "#llm_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
